# Spark-Modelos_de_regressao

**Data Analysis with PySpark:** Processing, Modeling, and Cross-Validation <br>
This project presents an introductory pipeline for data analysis and processing using PySpark, including steps for data cleaning, transformation, predictive modeling, and cross-validation using three algorithms: Linear Regression, Decision Tree, and Random Forest.<br><br>

**Technologies Used** <br>
Apache Spark: PySpark, SparkSQL, and MLlib <br>
Python <br>
Jupyter Notebook <br>
Pandas <br>
Matplotlib and Seaborn <br><br>

**Project Steps** <br>
1- Exploratory Data Analysis and Processing <br>
2- Data loading using SparkSession <br>
3- Handling of null values, removal of duplicates <br>
4- Type conversion and column transformation using StringIndexer, VectorAssembler, and StandardScaler <br>
5- Correlation checks, descriptive statistics, and outlier treatment <br><br>

**Objective**<br>
Demonstrate the first practical steps on how data pipelines can be structured using PySpark to handle large volumes of data and apply machine learning algorithms with performance, scalability, and data analysis and visualization methods.
